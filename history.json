[
    {
      "role": "user",
      "parts": [
        "{\".\\auto_commenter.py\": \"import os\nimport argparse\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\nfrom colorama import Fore\nimport json\n\nload_dotenv()\n\n# This script utilizes the Google Generative AI API to automatically add comments to provided Python code files. It supports both single file and directory processing. \ndef parse():\n    '''Parses command line arguments for file or directory input.\n    Args:\n        None\n    Returns:\n        str: file path if provided.\n        str: directory path if provided. '''\n    parser = argparse.ArgumentParser(description=\"Auto commenter\")\n    parser.add_argument(\n        \"--file\",\n        type=str,\n        required=False,\n        default=None,\n        help=\"File to be commented\",\n    )\n    parser.add_argument(\n        \"--directory\",\n        type=str,\n        required=False,\n        default=None,\n        help=\"Directory to be commented\",\n    )\n    args = parser.parse_args()\n    filename = args.file\n    directory = args.directory\n    return filename, directory\n\ndef llm_init():\n    '''Initializes the Google Generative AI model and starts a chat session.\n    Args:\n        None\n    Returns:\n        genai.Chat: Chat session object for interacting with the model. '''\n    with open(\"sys_instruction.txt\", \"r\") as instruction_file:\n        sys_instruction = instruction_file.read()\n    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GOOGLE_API_KEY)\n    model = genai.GenerativeModel(\n        model_name=\"gemini-1.5-flash\", system_instruction=sys_instruction\n    )\n    chat = model.start_chat(history=[])\n    return chat\n\ndef commenter(chat, file):\n    '''Sends the code to the LLM for commenting and retrieves the commented code.\n    Args:\n        genai.Chat: Chat session object for interacting with the model.\n        str: Code as a string.\n    Returns:\n        str: Commented code. '''\n    response = chat.send_message(file)\n    code_with_comments = response.text\n    return code_with_comments\n\ndef content_generator(file_name):\n    '''Reads the content of a file and formats it for LLM input.\n    Args:\n        str: File path.\n    Returns:\n        str: JSON string containing the file path and content. '''\n    with open(file_name, \"r\") as file:\n        content = file.read()\n    json_content = json.dumps({file_name: content})\n    return json_content\n\ndef content_writer(directory, commented_code):\n    '''Writes commented code to a file.\n    Args:\n        str: File path.\n        str: Commented Python code.\n    Returns:\n        None '''\n    with open(directory, \"w\") as file:\n        file.write(commented_code)\n\ndef extension_extractor(filename: str):\n    '''Extracts the extension of a file.\n    Args:\n        str: File path.\n    Returns:\n        str: File extension. '''\n    name_ext = filename.split(\".\")\n    return name_ext[-1]\n\ndef directory_commenter(root_dir):\n    '''Recursively traverses a directory and comments all supported code files.\n    Args:\n        str: Directory path.\n    Returns:\n        None '''\n    chat = llm_init()\n    ignore = [\"venv\", \".git\", \"__pycache__\"]\n    codes = [\"c\", \"cpp\", \"py\", \"js\", \"ts\", \"java\", \"rs\"]\n    entries = [\n        folder\n        for folder in os.listdir(root_dir)\n        if folder not in ignore and extension_extractor(folder) in codes\n    ]\n    entries = sorted(entries)\n\n    for iter, entry in enumerate(entries):\n        path = os.path.join(root_dir, entry)\n\n        if os.path.isdir(path):\n            directory_commenter(path)\n        else:\n            content = content_generator(path)\n            print(f\"{Fore.GREEN}Processing {path}...{Fore.WHITE}\")\n            commented = commenter(chat, content)\n            content_writer(path, commented)\n\ndef main():\n    '''Main function that orchestrates single file or directory commenting.\n    Args:\n        None\n    Returns:\n        None '''\n    chat = llm_init()\n\n    single_file, directory = parse()\n\n    if not single_file:\n        directory_commenter(directory)\n    else:\n        content = content_generator(single_file)\n        print(f\"{Fore.GREEN}Processing {single_file}...{Fore.WHITE}\")\n        commented = commenter(chat, content)\n        # Removing the initial \"```python\" and the last \"```\" from the code, as these are added by the LLM\n        if commented[:9] == \"```python\":\n            commented = commented[10:-3]\n        with open(single_file, \"w\") as code_file:\n            code_file.write(commented)\n\n\nmain()\n\"}"
      ]
    },
    {
      "role": "model",
      "parts": [
        "# Auto-Commenter\n\nThis script leverages the Google Generative AI API to automatically add comments to Python code files. It can process both individual files and entire directories.\n\n## Features\n\n- **Single File Processing:**  Comment a single Python file.\n- **Directory Traversal:** Recursively comment all supported code files within a directory.\n- **Google Generative AI Integration:** Uses the Gemini Pro model for accurate and context-aware comment generation.\n- **Customizable System Instructions:** Allows tailoring the commenting style through a `sys_instruction.txt` file.\n- **Support for multiple languages:** Supports c, cpp, py, js, ts, java, and rs.\n\n## Requirements\n\n- **Python:** Ensure you have Python installed.\n- **Google Cloud Project:** Set up a Google Cloud Project and enable the Generative AI API.\n- **API Key:** Obtain a Google API key and store it securely in a `.env` file as `GOOGLE_API_KEY`.\n- **Required Packages:** Install the necessary packages:\n\n  ```bash\n  pip install google-generativeai python-dotenv colorama\n  ```\n\n## Usage\n\n### Commenting a Single File\n\n```bash\npython auto_commenter.py --file <path_to_your_file.py>\n```\n\n### Commenting an Entire Directory\n\n```bash\npython auto_commenter.py --directory <path_to_your_directory>\n```\n\n## Configuration\n\n- **`sys_instruction.txt`:** This file contains instructions for the AI model, allowing you to customize the commenting style and behavior.\n\n## How it Works\n\n1. **Initialization:** The script initializes the Google Generative AI model and starts a chat session.\n2. **Parsing Arguments:** It parses command-line arguments to determine whether to process a single file or a directory.\n3. **File/Directory Processing:**\n   - **Single File:** Reads the file content, sends it to the AI model for commenting, and writes the commented code back to the file.\n   - **Directory:** Recursively traverses the directory, commenting each supported code file.\n4. **Comment Generation:** The AI model generates comments based on the code and the provided system instructions.\n5. **Output:** The commented code is either written back to the original file or saved to a new file.\n\n## Example `sys_instruction.txt`\n\n```\nYou are a helpful AI assistant that adds comments to code.\nBe comprehensive and explain what each part of the code does.\n```\n\n## Important Notes\n\n- Ensure your `.env` file contains the `GOOGLE_API_KEY`.\n- The script currently supports Python files. Support for other languages may be added in the future.\n- The quality of comments depends on the clarity and structure of the code.\n- Review the generated comments to ensure accuracy and completeness.\n- Remember to configure billing for your Google Cloud Project to avoid unexpected charges.\n"
      ]
    },
    {
      "role": "user",
      "parts": [
        "{\".\\linear_regression.py\": \"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json\nimport random\n# This code performs a linear regression on a given dataset of 'red' values to obtain a linear relationship between 'x' and 'y'. The code then calculates the accuracy of the model and visualizes the data and the regression line. \n# Extracting data\n\ndef dataset(location):\n    '''Loads the dataset from a JSON file and returns a dictionary containing the number of data points (m), x values, and y values.\n    Args:\n        location: Path to the JSON file containing the dataset.\n    Returns:\n        dict: Dictionary containing 'm', 'x', and 'y' values.\n    '''\n    dset = np.array(json.load(open(location))['red'])\n    return {'m': len(dset), 'x': dset[:, 0], 'y': dset[:, 1]}\n\n# Gradient Descent\n\ndef gradient_descent(x, y, m):\n    '''Performs gradient descent to find the optimal values for w and b in the linear equation y = wx + b.\n    Args:\n        x: Array of x values.\n        y: Array of y values.\n        m: Number of data points.\n    Returns:\n        dict: Dictionary containing the history of w, b, and the cost function (J) values during the gradient descent.\n    '''\n    w = random.random()\n    b = random.random()\n    init_w = w\n    init_b = b\n    learning_rate = 0.002\n    y_ = w*x+b\n    stats = {'w': [w], 'b': [b], 'J': [np.sum((y_-y)**2)/(2*m)]}\n    for i in range(125400):\n        # Update w and b using gradient descent formulas.\n        tmp_w = w - learning_rate * np.sum((y_-y)*x)/m\n        tmp_b = b - learning_rate * np.sum(y_-y)/m\n        w = tmp_w\n        b = tmp_b\n        y_ = w*x+b\n        J = np.sum((y_-y)**2)/(2*m)\n        stats['w'].append(w)\n        stats['b'].append(b)\n        stats['J'].append(J)\n    return stats\n\n# Running program\nlocation = \"../dataset-maker/dataset.json\"\nload_data = dataset(location)\nx = load_data['x']\ny = load_data['y']\nm = load_data['m']\nstats = gradient_descent(x, y, m)\ninit_w = stats['w'][0]\ninit_b = stats['b'][0]\nw = stats['w'][-1]\nb = stats['b'][-1]\nprint('Min cost', min(stats['J']))\nprint('Initial w:', init_w)\nprint('Final w:', w)\nprint('Initial b:', init_b)\nprint('Final b', b)\n\n# Testing Accuracy\naccuracy = 0\nfor i in range(m):\n    calculated_y = w*x[i]+b\n    accuracy += abs(calculated_y-y[i])*100/y[i]\naccuracy = 100-accuracy/m\nprint(f\"Accuracy: {accuracy}%\")\n\n# Data Visualization\nplx = np.linspace(0, 45, m)\nply = w*plx+b\nply_init = init_w*plx+init_b\nplt.figure(1)\nplt.xlabel(\"w\", color='red')\nplt.ylabel(\"J\", color='red')\nplt.scatter(stats['w'], stats['J'])\nplt.title('Gradient descent')\n\nplt.figure(2)\nplt.xlabel('x', color='red')\nplt.ylabel('y', color='red')\nplt.plot(plx, ply_init, color='blue', label='Before Linear Regression')\nplt.plot(plx, ply, color='red', label='After Linear Regression')\nplt.scatter(x, y)\nplt.legend()\nplt.show()\n\"}"
      ]
    },
    {
      "role": "model",
      "parts": [
        "# Linear Regression with Gradient Descent\n\nThis code implements linear regression using gradient descent to find the best-fit line for a given dataset. It visualizes the data points and the regression line, and also calculates the accuracy of the model.\n\n## Features\n\n* **Data Loading:** Loads data from a JSON file containing 'red' values representing (x, y) coordinates.\n* **Gradient Descent:** Implements the gradient descent algorithm to iteratively find the optimal values for the slope (w) and y-intercept (b) of the regression line.\n* **Accuracy Calculation:** Measures the accuracy of the model by comparing predicted y-values with actual y-values.\n* **Visualization:** Plots the data points, the initial regression line, and the final regression line after applying gradient descent.\n\n## Requirements\n\n* **Python:** Ensure you have Python installed.\n* **Libraries:** Install the necessary libraries using the following command:\n```bash\npip install numpy matplotlib pandas\n```\n\n## Usage\n\n1. **Dataset:** Prepare a JSON file containing an array of 'red' values, where each element is a list representing [x, y] coordinates.\n2. **File Path:** Update the `location` variable in the code to point to the path of your JSON dataset file.\n3. **Run the script:** Execute the Python script.\n\n## How it Works\n\n1. **`dataset(location)`:**\n   * Loads data from the specified JSON file.\n   * Extracts the x and y values from the 'red' array.\n   * Returns a dictionary containing the number of data points (`m`), x values, and y values.\n\n2. **`gradient_descent(x, y, m)`:**\n   * Initializes random values for `w` and `b`.\n   * Sets a learning rate for gradient descent.\n   * Iteratively updates `w` and `b` using the gradient descent formulas to minimize the cost function (mean squared error).\n   * Stores the history of `w`, `b`, and the cost function (`J`) values during the iterations.\n   * Returns a dictionary containing the history of these values.\n\n3. **Main Execution:**\n   * Loads the data using `dataset(location)`.\n   * Performs gradient descent using `gradient_descent(x, y, m)`.\n   * Prints the initial and final values of `w` and `b`, as well as the minimum cost achieved.\n   * Calculates the accuracy of the model.\n   * Generates plots to visualize the gradient descent process and the regression lines.\n\n## Visualization\n\n* **Figure 1:** Shows the relationship between `w` (slope) and `J` (cost function) during gradient descent.\n* **Figure 2:** Displays the scatter plot of the data points, the initial regression line (blue), and the final regression line (red) after applying gradient descent.\n\n## Example Dataset (dataset.json)\n\n```json\n{\n  \"red\": [\n    [1, 2],\n    [2, 4],\n    [3, 5],\n    [4, 7],\n    [5, 8]\n  ]\n}\n```\n\n## Notes\n\n* The learning rate and number of iterations in gradient descent can be adjusted for better performance.\n* The accuracy calculation method can be modified based on the specific requirements.\n* This code serves as a basic example and can be further enhanced with features like data normalization, regularization, and more sophisticated accuracy metrics.\n"
      ]
    },
    {
      "role": "user",
      "parts": [
        "{\".\\polynomial_regression.py\": \"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json\nimport random\n# This code performs a polynomial regression on a given dataset of 'red' values to obtain a polynomial relationship between 'x' and 'y' of the form y = w0x^2 + w1x + b. The code then calculates the accuracy of the model and visualizes the data and the regression curve. \n# Extracting data\n\ndef dataset(location):\n    '''Loads the dataset from a JSON file and returns a dictionary containing the number of data points (m), x values, and y values.\n    Args:\n        location: Path to the JSON file containing the dataset.\n    Returns:\n        dict: Dictionary containing 'm', 'x', and 'y' values.\n    '''\n    dset = np.array(json.load(open(location))['red'])\n    return {'m': len(dset), 'x': dset[:, 0], 'y': dset[:, 1]}\n\n# Gradient Descent\ndef gradient_descent(x, y, m):\n    '''Performs gradient descent to find the optimal values for w0, w1, and b in the polynomial equation y = w0x^2 + w1x + b.\n    Args:\n        x: Array of x values.\n        y: Array of y values.\n        m: Number of data points.\n    Returns:\n        dict: Dictionary containing the history of w0, w1, b, and the cost function (J) values during the gradient descent.\n    '''\n    w0 = random.randint(0,4)\n    w1 = random.random()\n    b = random.random()\n    learning_rate = 0.00000293\n    y_ = w0*(x**2)+w1*x+b\n    stats = {'w0': [w0], 'w1': [w1], 'b': [b], 'J': [np.sum((y_-y)**2)/(2*m)]}\n    for i in range(146400):\n        # Update w0, w1, and b using gradient descent formulas.\n        tmp_w0 = w0 - learning_rate * np.sum((y_-y)*(x**2))/m\n        tmp_w1 = w1 - learning_rate * np.sum((y_-y)*x)/m\n        tmp_b = b - learning_rate * np.sum(y_-y)/m\n        w0 = tmp_w0\n        w1 = tmp_w1\n        b = tmp_b\n        y_ = w0*(x**2)+w1*x+b\n        J = np.sum((y_-y)**2)\n        J = J/(2*m)\n        stats['w0'].append(w0)\n        stats['w1'].append(w1)\n        stats['b'].append(b)\n        stats['J'].append(J)\n    return stats\n\n# Running program\nlocation = '../dataset-maker/dataset.json'\nload_data = dataset(location)\nx = load_data['x']\ny = load_data['y']\nm = load_data['m']\nstats = gradient_descent(x, y, m)\ninit_w0 = stats['w0'][0]\ninit_w1 = stats['w1'][0]\ninit_b = stats['b'][0]\nw0 = stats['w0'][-1]\nw1 = stats['w1'][-1]\nb = stats['b'][-1]\nprint('Min cost', min(stats['J']))\nprint('Initial w:', init_w0)\nprint('Initial w1:',init_w1)\nprint('Final w0:', w0)\nprint('Final w1:',w1)\nprint('Initial b:', init_b)\nprint('Final b', b)\n\n# Testing Accuracy\naccuracy = 0\nfor i in range(m):\n    calculated_y = w0*(x[i]**2)+w1*x[i]+b\n    accuracy += abs(calculated_y-y[i])*100/y[i]\naccuracy = 100-accuracy/m\nprint(f\"Accuracy: {accuracy}%\")\n\n# Data Visualization\nplx = np.linspace(0, 45, m)\nply = np.polyval([w0,w1,b], plx)\nply_init = init_w0*plx+init_b\nplt.figure(1)\nplt.xlabel(\"w\", color='red')\nplt.ylabel(\"J\", color='red')\nplt.scatter(stats['w0'], stats['J'])\nplt.scatter(stats['w0'][-1],stats['J'][-1], color='red')\nplt.title('Gradient descent')\n\nplt.figure(2)\nplt.xlabel('x', color='red')\nplt.ylabel('y', color='red')\nplt.plot(plx, ply_init, color='blue', label='Before Polynomial Regression')\nplt.plot(plx, ply, color='red', label='After Polynomial Regression')\nplt.scatter(x, y)\nplt.legend()\nplt.show()\"}"
      ]
    },
    {
      "role": "model",
      "parts": [
        "# Polynomial Regression with Gradient Descent\n\nThis code implements polynomial regression using gradient descent to find the best-fit curve for a given dataset. It visualizes the data points and the regression curve, and also calculates the accuracy of the model.\n\n## Features\n\n* **Data Loading:** Loads data from a JSON file containing 'red' values representing (x, y) coordinates.\n* **Gradient Descent:** Implements the gradient descent algorithm to iteratively find the optimal values for the coefficients (w0, w1, b) of the polynomial equation y = w0x^2 + w1x + b.\n* **Accuracy Calculation:** Measures the accuracy of the model by comparing predicted y-values with actual y-values.\n* **Visualization:** Plots the data points, the initial regression line, and the final regression curve after applying gradient descent.\n\n## Requirements\n\n* **Python:** Ensure you have Python installed.\n* **Libraries:** Install the necessary libraries using the following command:\n```bash\npip install numpy matplotlib pandas\n```\n\n## Usage\n\n1. **Dataset:** Prepare a JSON file containing an array of 'red' values, where each element is a list representing [x, y] coordinates.\n2. **File Path:** Update the `location` variable in the code to point to the path of your JSON dataset file.\n3. **Run the script:** Execute the Python script.\n\n## How it Works\n\n1. **`dataset(location)`:**\n   * Loads data from the specified JSON file.\n   * Extracts the x and y values from the 'red' array.\n   * Returns a dictionary containing the number of data points (`m`), x values, and y values.\n\n2. **`gradient_descent(x, y, m)`:**\n   * Initializes random values for `w0`, `w1`, and `b`.\n   * Sets a learning rate for gradient descent.\n   * Iteratively updates `w0`, `w1`, and `b` using the gradient descent formulas to minimize the cost function (mean squared error).\n   * Stores the history of `w0`, `w1`, `b`, and the cost function (`J`) values during the iterations.\n   * Returns a dictionary containing the history of these values.\n\n3. **Main Execution:**\n   * Loads the data using `dataset(location)`.\n   * Performs gradient descent using `gradient_descent(x, y, m)`.\n   * Prints the initial and final values of `w0`, `w1`, and `b`, as well as the minimum cost achieved.\n   * Calculates the accuracy of the model.\n   * Generates plots to visualize the gradient descent process and the regression curves.\n\n## Visualization\n\n* **Figure 1:** Shows the relationship between `w0` (coefficient of x^2) and `J` (cost function) during gradient descent.\n* **Figure 2:** Displays the scatter plot of the data points, the initial regression line (blue), and the final regression curve (red) after applying gradient descent.\n\n## Example Dataset (dataset.json)\n\n```json\n{\n  \"red\": [\n    [1, 2],\n    [2, 5],\n    [3, 10],\n    [4, 17],\n    [5, 26]\n  ]\n}\n```\n\n## Notes\n\n* The learning rate and number of iterations in gradient descent can be adjusted for better performance.\n* The accuracy calculation method can be modified based on the specific requirements.\n* This code serves as a basic example and can be further enhanced with features like data normalization, regularization, and more sophisticated accuracy metrics.\n* The polynomial degree can be adjusted by modifying the equation used in `gradient_descent` and the plotting section.\n"
      ]
    }
  ]